{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3563a2a9",
   "metadata": {},
   "source": [
    "# Interpretación de texto  \n",
    "En este notebook vemos como podemos empezar a interpretar texto. \n",
    "Los análisis se basan  en información de las palabras guardadas en largas bases de datos. Estas bases de texto se llaman **corpus**. Un corpus es un conjunto de textos o de palabras que se han recopilado y que suelen ir acompañados de anotaciones como categoría léxica, definición, etc.  \n",
    "\n",
    "Con esa información también se entrenan modelos de ML para evaluar palabras fuera del corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ee3cfb",
   "metadata": {},
   "source": [
    "## 1. Tagging\n",
    "Para empezar a procesar texto y dilucidar su significado nos puede convenir identificar a qué categoría gramatical pertenece cada palabra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec84092d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Magnus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('cosmic', 'JJ'),\n",
       " ('microwave', 'NN'),\n",
       " ('background', 'NN'),\n",
       " ('(', '('),\n",
       " ('CMB', 'NNP'),\n",
       " (',', ','),\n",
       " ('CMBR', 'NNP'),\n",
       " (')', ')'),\n",
       " (',', ','),\n",
       " ('in', 'IN'),\n",
       " ('Big', 'NNP'),\n",
       " ('Bang', 'NNP'),\n",
       " ('cosmology', 'NN'),\n",
       " (',', ','),\n",
       " ('is', 'VBZ'),\n",
       " ('electromagnetic', 'JJ'),\n",
       " ('radiation', 'NN'),\n",
       " ('which', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('remnant', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('early', 'JJ'),\n",
       " ('stage', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('universe', 'NN'),\n",
       " (',', ','),\n",
       " ('also', 'RB'),\n",
       " ('known', 'VBN'),\n",
       " ('as', 'IN'),\n",
       " ('``', '``'),\n",
       " ('relic', 'JJ'),\n",
       " ('radiation', 'NN'),\n",
       " (\"''\", \"''\"),\n",
       " ('.', '.'),\n",
       " ('[', 'VB'),\n",
       " ('1', 'CD'),\n",
       " (']', 'IN'),\n",
       " ('The', 'DT'),\n",
       " ('CMB', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('faint', 'JJ'),\n",
       " ('cosmic', 'JJ'),\n",
       " ('background', 'NN'),\n",
       " ('radiation', 'NN'),\n",
       " ('filling', 'VBG'),\n",
       " ('all', 'DT'),\n",
       " ('space', 'NN'),\n",
       " ('.', '.'),\n",
       " ('It', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('an', 'DT'),\n",
       " ('important', 'JJ'),\n",
       " ('source', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('data', 'NNS'),\n",
       " ('on', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('early', 'JJ'),\n",
       " ('universe', 'NN'),\n",
       " ('because', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('oldest', 'JJS'),\n",
       " ('electromagnetic', 'JJ'),\n",
       " ('radiation', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('universe', 'NN'),\n",
       " (',', ','),\n",
       " ('dating', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('epoch', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('recombination', 'NN'),\n",
       " ('.', '.'),\n",
       " ('With', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('traditional', 'JJ'),\n",
       " ('optical', 'JJ'),\n",
       " ('telescope', 'NN'),\n",
       " (',', ','),\n",
       " ('the', 'DT'),\n",
       " ('space', 'NN'),\n",
       " ('between', 'IN'),\n",
       " ('stars', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('galaxies', 'NNS'),\n",
       " ('(', '('),\n",
       " ('the', 'DT'),\n",
       " ('background', 'NN'),\n",
       " (')', ')'),\n",
       " ('is', 'VBZ'),\n",
       " ('completely', 'RB'),\n",
       " ('dark', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('However', 'RB'),\n",
       " (',', ','),\n",
       " ('a', 'DT'),\n",
       " ('sufficiently', 'RB'),\n",
       " ('sensitive', 'JJ'),\n",
       " ('radio', 'NN'),\n",
       " ('telescope', 'NN'),\n",
       " ('shows', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('faint', 'NN'),\n",
       " ('background', 'NN'),\n",
       " ('noise', 'NN'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('glow', 'NN'),\n",
       " (',', ','),\n",
       " ('almost', 'RB'),\n",
       " ('isotropic', 'NN'),\n",
       " (',', ','),\n",
       " ('that', 'WDT'),\n",
       " ('is', 'VBZ'),\n",
       " ('not', 'RB'),\n",
       " ('associated', 'VBN'),\n",
       " ('with', 'IN'),\n",
       " ('any', 'DT'),\n",
       " ('star', 'NN'),\n",
       " (',', ','),\n",
       " ('galaxy', 'NN'),\n",
       " (',', ','),\n",
       " ('or', 'CC'),\n",
       " ('other', 'JJ'),\n",
       " ('object', 'NN'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('glow', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('strongest', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('microwave', 'JJ'),\n",
       " ('region', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('radio', 'NN'),\n",
       " ('spectrum', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('accidental', 'JJ'),\n",
       " ('discovery', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('CMB', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('1965', 'CD'),\n",
       " ('by', 'IN'),\n",
       " ('American', 'JJ'),\n",
       " ('radio', 'NN'),\n",
       " ('astronomers', 'NNS'),\n",
       " ('Arno', 'NNP'),\n",
       " ('Penzias', 'NNP'),\n",
       " ('and', 'CC'),\n",
       " ('Robert', 'NNP'),\n",
       " ('Wilson', 'NNP'),\n",
       " ('[', 'VBD'),\n",
       " ('2', 'CD'),\n",
       " (']', 'NNP'),\n",
       " ('[', 'VBD'),\n",
       " ('3', 'CD'),\n",
       " (']', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('culmination', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('work', 'NN'),\n",
       " ('initiated', 'VBN'),\n",
       " ('in', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('1940s', 'CD'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('earned', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('discoverers', 'NNS'),\n",
       " ('the', 'DT'),\n",
       " ('1978', 'CD'),\n",
       " ('Nobel', 'NNP'),\n",
       " ('Prize', 'NNP'),\n",
       " ('in', 'IN'),\n",
       " ('Physics', 'NNP'),\n",
       " ('.', '.'),\n",
       " ('CMB', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('landmark', 'JJ'),\n",
       " ('evidence', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('Big', 'NNP'),\n",
       " ('Bang', 'NNP'),\n",
       " ('origin', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('universe', 'NN'),\n",
       " ('.', '.'),\n",
       " ('When', 'WRB'),\n",
       " ('the', 'DT'),\n",
       " ('universe', 'NN'),\n",
       " ('was', 'VBD'),\n",
       " ('young', 'JJ'),\n",
       " (',', ','),\n",
       " ('before', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('formation', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('stars', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('planets', 'NNS'),\n",
       " (',', ','),\n",
       " ('it', 'PRP'),\n",
       " ('was', 'VBD'),\n",
       " ('denser', 'RB'),\n",
       " (',', ','),\n",
       " ('much', 'JJ'),\n",
       " ('hotter', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('filled', 'VBD'),\n",
       " ('with', 'IN'),\n",
       " ('an', 'DT'),\n",
       " ('opaque', 'JJ'),\n",
       " ('fog', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('hydrogen', 'NN'),\n",
       " ('plasma', 'NN'),\n",
       " ('.', '.'),\n",
       " ('As', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('universe', 'NN'),\n",
       " ('expanded', 'VBD'),\n",
       " ('the', 'DT'),\n",
       " ('plasma', 'NN'),\n",
       " ('grew', 'VBD'),\n",
       " ('cooler', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('radiation', 'NN'),\n",
       " ('filling', 'VBG'),\n",
       " ('it', 'PRP'),\n",
       " ('expanded', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('longer', 'JJR'),\n",
       " ('wavelengths', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('When', 'WRB'),\n",
       " ('the', 'DT'),\n",
       " ('temperature', 'NN'),\n",
       " ('had', 'VBD'),\n",
       " ('dropped', 'VBN'),\n",
       " ('enough', 'RB'),\n",
       " (',', ','),\n",
       " ('protons', 'NNS'),\n",
       " ('and', 'CC'),\n",
       " ('electrons', 'NNS'),\n",
       " ('combined', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('form', 'VB'),\n",
       " ('neutral', 'JJ'),\n",
       " ('hydrogen', 'NN'),\n",
       " ('atoms', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('Unlike', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('plasma', 'NN'),\n",
       " (',', ','),\n",
       " ('these', 'DT'),\n",
       " ('newly', 'RB'),\n",
       " ('conceived', 'VBN'),\n",
       " ('atoms', 'NNS'),\n",
       " ('could', 'MD'),\n",
       " ('not', 'RB'),\n",
       " ('scatter', 'VB'),\n",
       " ('the', 'DT'),\n",
       " ('thermal', 'JJ'),\n",
       " ('radiation', 'NN'),\n",
       " ('by', 'IN'),\n",
       " ('Thomson', 'NNP'),\n",
       " ('scattering', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('so', 'RB'),\n",
       " ('the', 'DT'),\n",
       " ('universe', 'NN'),\n",
       " ('became', 'VBD'),\n",
       " ('transparent', 'JJ'),\n",
       " ('.', '.'),\n",
       " ('[', '$'),\n",
       " ('4', 'CD'),\n",
       " (']', 'JJ'),\n",
       " ('Cosmologists', 'NNS'),\n",
       " ('refer', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('time', 'NN'),\n",
       " ('period', 'NN'),\n",
       " ('when', 'WRB'),\n",
       " ('neutral', 'JJ'),\n",
       " ('atoms', 'NNS'),\n",
       " ('first', 'RB'),\n",
       " ('formed', 'VBN'),\n",
       " ('as', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('recombination', 'NN'),\n",
       " ('epoch', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('the', 'DT'),\n",
       " ('event', 'NN'),\n",
       " ('shortly', 'RB'),\n",
       " ('afterwards', 'NNS'),\n",
       " ('when', 'WRB'),\n",
       " ('photons', 'NNS'),\n",
       " ('started', 'VBD'),\n",
       " ('to', 'TO'),\n",
       " ('travel', 'VB'),\n",
       " ('freely', 'RB'),\n",
       " ('through', 'IN'),\n",
       " ('space', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('referred', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('as', 'IN'),\n",
       " ('photon', 'NN'),\n",
       " ('decoupling', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('photons', 'NNS'),\n",
       " ('that', 'WDT'),\n",
       " ('existed', 'VBD'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('time', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('photon', 'NN'),\n",
       " ('decoupling', 'NN'),\n",
       " ('have', 'VBP'),\n",
       " ('been', 'VBN'),\n",
       " ('propagating', 'VBG'),\n",
       " ('ever', 'RB'),\n",
       " ('since', 'IN'),\n",
       " (',', ','),\n",
       " ('though', 'IN'),\n",
       " ('growing', 'VBG'),\n",
       " ('less', 'RBR'),\n",
       " ('energetic', 'JJ'),\n",
       " (',', ','),\n",
       " ('since', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('expansion', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('space', 'NN'),\n",
       " ('causes', 'NNS'),\n",
       " ('their', 'PRP$'),\n",
       " ('wavelength', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('increase', 'VB'),\n",
       " ('over', 'IN'),\n",
       " ('time', 'NN'),\n",
       " ('(', '('),\n",
       " ('and', 'CC'),\n",
       " ('wavelength', 'NN'),\n",
       " ('is', 'VBZ'),\n",
       " ('inversely', 'RB'),\n",
       " ('proportional', 'JJ'),\n",
       " ('to', 'TO'),\n",
       " ('energy', 'NN'),\n",
       " ('according', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('Planck', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('relation', 'NN'),\n",
       " (')', ')'),\n",
       " ('.', '.'),\n",
       " ('This', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('the', 'DT'),\n",
       " ('source', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('alternative', 'JJ'),\n",
       " ('term', 'NN'),\n",
       " ('relic', 'JJ'),\n",
       " ('radiation', 'NN'),\n",
       " ('.', '.'),\n",
       " ('The', 'DT'),\n",
       " ('surface', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('last', 'JJ'),\n",
       " ('scattering', 'VBG'),\n",
       " ('refers', 'NNS'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('set', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('points', 'NNS'),\n",
       " ('in', 'IN'),\n",
       " ('space', 'NN'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('right', 'JJ'),\n",
       " ('distance', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('us', 'PRP'),\n",
       " ('so', 'RB'),\n",
       " ('that', 'IN'),\n",
       " ('we', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('now', 'RB'),\n",
       " ('receiving', 'VBG'),\n",
       " ('photons', 'NNS'),\n",
       " ('originally', 'RB'),\n",
       " ('emitted', 'VBN'),\n",
       " ('from', 'IN'),\n",
       " ('those', 'DT'),\n",
       " ('points', 'NNS'),\n",
       " ('at', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('time', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('photon', 'NN'),\n",
       " ('decoupling', 'NN')]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "f = open('../data/text2.txt','r')\n",
    "text = f.read()\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "# nos da la categoría léxica de cada palabra\n",
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b42223",
   "metadata": {},
   "source": [
    "En este [link](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) podéis ver el significado de cada tag. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7937bac",
   "metadata": {},
   "source": [
    "## 2. Name Entity Recognition\n",
    "También nos puede ser útil identificar los **nombres** en distintas categorías, por ejemplo: nombre propio, lugar, fecha\n",
    "con el paquete `SpaCy`. Este paquete es interesantes para NLP también. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c791e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "     ---------------------------------------- 12.8/12.8 MB 3.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.64.1)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (65.6.3)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.6)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.11)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (22.0)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2023.5.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.6.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "Collecting es-core-news-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/es_core_news_sm-3.6.0/es_core_news_sm-3.6.0-py3-none-any.whl (12.9 MB)\n",
      "     ---------------------------------------- 12.9/12.9 MB 4.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from es-core-news-sm==3.6.0) (3.6.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.10.11)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (5.2.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.8)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.64.1)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.4.6)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.1.10)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (65.6.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.28.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (22.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.23.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (4.4.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2023.5.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (1.26.14)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.1.0)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.7.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\magnus\\anaconda3\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->es-core-news-sm==3.6.0) (2.1.1)\n",
      "Installing collected packages: es-core-news-sm\n",
      "Successfully installed es-core-news-sm-3.6.0\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('es_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "#!conda install -c conda-forge spacy\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download es_core_news_sm\n",
    "\n",
    "\n",
    "NER = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "bbacac87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMB ORG\n",
      "CMBR ORG\n",
      "Big Bang GPE\n",
      "CMB ORG\n",
      "CMB ORG\n",
      "1965 DATE\n",
      "American NORP\n",
      "Arno Penzias PERSON\n",
      "Robert PERSON\n",
      "the 1940s DATE\n",
      "1978 DATE\n",
      "Nobel Prize in Physics WORK_OF_ART\n",
      "CMB ORG\n",
      "Thomson ORG\n",
      "first ORDINAL\n",
      "wavelength ORG\n",
      "Planck ORG\n"
     ]
    }
   ],
   "source": [
    "text1 = NER(text)\n",
    "\n",
    "for word in text1.ents:\n",
    "    print(word.text,word.label_)\n",
    "    \n",
    "# Ents son las entidades que según el modelo entrenado (en_core_web_sm) lo son, \n",
    "# puede haber errores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db2fcb5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(CMB,\n",
       " CMBR,\n",
       " Big Bang,\n",
       " CMB,\n",
       " CMB,\n",
       " 1965,\n",
       " American,\n",
       " Arno Penzias,\n",
       " Robert,\n",
       " the 1940s,\n",
       " 1978,\n",
       " Nobel Prize in Physics,\n",
       " CMB,\n",
       " Thomson,\n",
       " first,\n",
       " wavelength,\n",
       " Planck)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1.ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "50e4375f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The cosmic microwave background (\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CMB\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CMBR\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "), in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Big Bang\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " cosmology, is electromagnetic radiation which is a remnant from an early stage of the universe, also known as &quot;relic radiation&quot;.[1] The \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CMB\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is faint cosmic background radiation filling all space. It is an important source of data on the early universe because it is the oldest electromagnetic radiation in the universe, dating to the epoch of recombination. With a traditional optical telescope, the space between stars and galaxies (the background) is completely dark. However, a sufficiently sensitive radio telescope shows a faint background noise, or glow, almost isotropic, that is not associated with any star, galaxy, or other object. This glow is strongest in the microwave region of the radio spectrum. The accidental discovery of the \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CMB\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1965\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " by \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    American\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " radio astronomers \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Arno Penzias\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Robert\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " Wilson[2][3] was the culmination of work initiated in \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    the 1940s\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ", and earned the discoverers the \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    1978\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #f0d0ff; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Nobel Prize in Physics\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">WORK_OF_ART</span>\n",
       "</mark>\n",
       ".</br></br>\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    CMB\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is landmark evidence of the Big Bang origin of the universe. When the universe was young, before the formation of stars and planets, it was denser, much hotter, and filled with an opaque fog of hydrogen plasma. As the universe expanded the plasma grew cooler and the radiation filling it expanded to longer wavelengths. When the temperature had dropped enough, protons and electrons combined to form neutral hydrogen atoms. Unlike the plasma, these newly conceived atoms could not scatter the thermal radiation by \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Thomson\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " scattering, and so the universe became transparent.[4] Cosmologists refer to the time period when neutral atoms \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    first\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " formed as the recombination epoch, and the event shortly afterwards when photons started to travel freely through space is referred to as photon decoupling. The photons that existed at the time of photon decoupling have been propagating ever since, though growing less energetic, since the expansion of space causes their wavelength to increase over time (and \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    wavelength\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is inversely proportional to energy according to \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Planck\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       "'s relation). This is the source of the alternative term relic radiation. The surface of last scattering refers to the set of points in space at the right distance from us so that we are now receiving photons originally emitted from those points at the time of photon decoupling</br></div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# lo podemos visualizar\n",
    "displacy.render(text1, style=\"ent\", jupyter=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "68018770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"first\", \"second\", etc.'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy.explain(\"ORDINAL\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b603e573",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple is looking at buying U.K. startup for $1 billion\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.en.examples import sentences  # ejemplos\n",
    "print(sentences[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4711ad1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Apple\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " is looking at buying \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    U.K.\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " startup for \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    $1 billion\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MONEY</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = NER(sentences[0])\n",
    "displacy.render(doc,style=\"ent\",jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512bbbd8",
   "metadata": {},
   "source": [
    "También podemos usar **Spacy** para más cosas como categorías gramaticales: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ee36830d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple PROPN\n",
      "is AUX\n",
      "looking VERB\n",
      "at ADP\n",
      "buying VERB\n",
      "U.K. PROPN\n",
      "startup NOUN\n",
      "for ADP\n",
      "$ SYM\n",
      "1 NUM\n",
      "billion NUM\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ebff4fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple\n",
      "is be\n",
      "looking look\n",
      "at at\n",
      "buying buy\n",
      "U.K. U.K.\n",
      "startup startup\n",
      "for for\n",
      "$ $\n",
      "1 1\n",
      "billion billion\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token.text, token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d4f59d2",
   "metadata": {},
   "source": [
    "## 3. Significado de las palabras\n",
    "Para saber el significado una palabra se usan bases de datos, en inglés el más extenso y utilizado es [wordnet](https://wordnet.princeton.edu). Lo que llamamos **corpus**. Sobre el corpus wordnet se ha trabajado mucho y hay mucha información de dichas palabras. Además se ha usado para entrenar modelos y predecir información de palabras que no estén en dicho corpus.\n",
    "\n",
    "Lo cargamos desde `nltk`y usamos la función `synsets` que nos devuelve una lista con las acepciones de una palabra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1162b7f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('drink.n.01'),\n",
       " Synset('drink.n.02'),\n",
       " Synset('beverage.n.01'),\n",
       " Synset('drink.n.04'),\n",
       " Synset('swallow.n.02'),\n",
       " Synset('drink.v.01'),\n",
       " Synset('drink.v.02'),\n",
       " Synset('toast.v.02'),\n",
       " Synset('drink_in.v.01'),\n",
       " Synset('drink.v.05')]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "wn.synsets('drink')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e4aef20a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'an adult female person (as opposed to a man)'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('woman.n.01').definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b4b12f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a female person who plays a significant role (wife or mistress or girlfriend) in the life of a particular man'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('woman.n.02').definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b22209b",
   "metadata": {},
   "source": [
    "También nos permite obtener una lista de **sinónimos** y **antónimos**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c601e15e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['woman', 'adult_female']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woman = wn.synset('woman.n.01')\n",
    "woman.lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "7ec2d843",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('man.n.01.man')]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woman.lemmas()[0].antonyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448b26f3",
   "metadata": {},
   "source": [
    "También podemos obtener **hyperónimos**, grupo al que pertenece la palabra y **hipónimos**, subgrupos o ejemplos específicos de dicha acepción de la palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "00a3cea2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('adult.n.01'), Synset('female.n.02')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woman.hypernyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9034b1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('amazon.n.01'),\n",
       " Synset('b-girl.n.01'),\n",
       " Synset('bachelor_girl.n.01'),\n",
       " Synset('baggage.n.02'),\n",
       " Synset('ball-buster.n.01'),\n",
       " Synset('black_woman.n.01'),\n",
       " Synset('bluestocking.n.01'),\n",
       " Synset('bridesmaid.n.01'),\n",
       " Synset('broad.n.01'),\n",
       " Synset('cat.n.03'),\n",
       " Synset('cinderella.n.01'),\n",
       " Synset('coquette.n.01'),\n",
       " Synset('dame.n.02'),\n",
       " Synset('debutante.n.01'),\n",
       " Synset('divorcee.n.01'),\n",
       " Synset('dominatrix.n.01'),\n",
       " Synset('donna.n.01'),\n",
       " Synset('enchantress.n.01'),\n",
       " Synset('ex-wife.n.01'),\n",
       " Synset('eyeful.n.01'),\n",
       " Synset('geisha.n.01'),\n",
       " Synset('girl.n.01'),\n",
       " Synset('girl.n.05'),\n",
       " Synset('girlfriend.n.01'),\n",
       " Synset('girlfriend.n.02'),\n",
       " Synset('gold_digger.n.02'),\n",
       " Synset('gravida.n.02'),\n",
       " Synset('heroine.n.02'),\n",
       " Synset('inamorata.n.01'),\n",
       " Synset('jezebel.n.02'),\n",
       " Synset('jilt.n.01'),\n",
       " Synset('lady.n.01'),\n",
       " Synset('maenad.n.01'),\n",
       " Synset('maenad.n.02'),\n",
       " Synset('matriarch.n.01'),\n",
       " Synset('matriarch.n.02'),\n",
       " Synset('matron.n.03'),\n",
       " Synset('mestiza.n.01'),\n",
       " Synset('mistress.n.01'),\n",
       " Synset('mother_figure.n.01'),\n",
       " Synset('nanny.n.01'),\n",
       " Synset('nullipara.n.01'),\n",
       " Synset('nymph.n.03'),\n",
       " Synset('nymphet.n.01'),\n",
       " Synset('old_woman.n.01'),\n",
       " Synset('prostitute.n.01'),\n",
       " Synset('shiksa.n.01'),\n",
       " Synset('smasher.n.02'),\n",
       " Synset('sylph.n.01'),\n",
       " Synset('unmarried_woman.n.01'),\n",
       " Synset('vestal.n.01'),\n",
       " Synset('wac.n.01'),\n",
       " Synset('wave.n.09'),\n",
       " Synset('white_woman.n.01'),\n",
       " Synset('widow.n.01'),\n",
       " Synset('wife.n.01'),\n",
       " Synset('wonder_woman.n.01'),\n",
       " Synset('yellow_woman.n.01')]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "woman.hyponyms()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cfd415",
   "metadata": {},
   "source": [
    "Esto te puede permitir encontrar relaciones entre palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "9cd9a4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('adult.n.01')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('woman.n.01').lowest_common_hypernyms(wn.synset('man.n.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbc07cbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('carnivore.n.01')]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('cat.n.01').lowest_common_hypernyms(wn.synset('dog.n.01'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "524cbeed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('organism.n.01')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synset('cat.n.01').lowest_common_hypernyms(wn.synset('daisy.n.01'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f9e9a2",
   "metadata": {},
   "source": [
    "## 4. Similitud entre palabras\n",
    "Podemos calcular como de parecidas son dos palabras, para eso tenemos algunas definiciones de distancias:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cdfef58",
   "metadata": {},
   "source": [
    "### 4.1 Comparación de carácteres\n",
    " - **Levenshtein distance.**\n",
    "Nos indica el número de ediciones que tenemos que hacer para pasar de una palabra a la otra. Es decir, los carácteres que tenemos que cambiar, añadir o quitar para pasar de una *string1* a *string2*. Es una distancia basada en los carácteres no tiene en cuenta la similitud de significado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7bddab52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.edit_distance(\"easy\", \"easily\")   # quitar l e y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8f658df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.edit_distance(\"make it easy\", \"make it smart\")  ## seay --> smay --> smar --> smart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3b65f7bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.edit_distance(\"calor\", \"colar\")  # un cambio a por o y otro por a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcddc25",
   "metadata": {},
   "source": [
    " -  **Jaro Similarity**\n",
    " Otra medida de similitud de carácteres que está entre 0 y 1 y depende de carácteres iguales (m), el número de transposiciones (2t) y la longitud de las *strings* (s1,s2). Es más interesante que la anterior para frases o palabras largas, porque va pesada por la longitud. \n",
    " \n",
    "<img src = img/jaro.png width = 400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "bbbd7ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8888888888888888"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics import distance as dist\n",
    "dist.jaro_similarity(\"easy\", \"easily\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "95827719",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8008547008547009"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.jaro_similarity(\"make it easy\", \"meak it smart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4297eacc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7333333333333334"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist.jaro_similarity(\"calor\", \"colar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66743f83",
   "metadata": {},
   "source": [
    "### 4.2 Similitud de significado palabras \n",
    "Para calcular la similitud en significado de dos palabras podemos usar distintas distancias:\n",
    "\n",
    " - **Path-similarity** \n",
    " \n",
    "devuelve valores entre 0 y 1, dependiendo del camino más corto que conecta los sentidos de dos palabras mirando a los hiperónimos e hipónimos. \n",
    "\n",
    "$ Path_similarity = 1 / (pasos + 1)$. \n",
    "\n",
    "1 paso  path_similarity = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cf4a5e",
   "metadata": {},
   "source": [
    "<img src = img/pasos.jpg /> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "48fd56d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "dog = wn.synset('dog.n.01')\n",
    "cat = wn.synset('cat.n.01')\n",
    "crab = wn.synset('crab.n.01')\n",
    "spoon = wn.synset('spoon.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d9e04b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "elephant = wn.synset('elephant.n.01')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2612c89a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "fb79da1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.125"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(crab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4cda9614",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(spoon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "63ec7b30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wolf = wn.synset('wolf.n.01')\n",
    "dog.path_similarity(wolf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ed8d5057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a young dog'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "puppy = wn.synset('puppy.n.01')\n",
    "puppy.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0110fb23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dog.path_similarity(puppy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a10ff66",
   "metadata": {},
   "source": [
    "  - **Wu-Palmer Similarity**\n",
    "  \n",
    "  Depende del tamaño de la clase a la que pertenezcan\n",
    "  \n",
    "  $ Wu = \\frac{2*depth(LCS)}{depth(s1)+depth(s2)}$ \n",
    "  \n",
    "  Depth sería el número de pasos desde arriba del árbol hasta la palabra. El árbol de wordnet se organiza más o menos así :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506758f",
   "metadata": {},
   "source": [
    "  <img src = img/WordNetTree.png width = 400 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "aed1a402",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'wu distance dog.n.01 with cat.n.01': 0.8571428571428571,\n",
       " 'wu distance dog.n.01 with crab.n.01': 0.6666666666666666,\n",
       " 'wu distance dog.n.01 with spoon.n.01': 0.47058823529411764,\n",
       " 'wu distance dog.n.01 with wolf.n.01': 0.9285714285714286,\n",
       " 'wu distance dog.n.01 with puppy.n.01': 0.896551724137931}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list2 = ['cat.n.01', 'crab.n.01', 'spoon.n.01', 'wolf.n.01', 'puppy.n.01']\n",
    "\n",
    "wu_dict = {'wu distance dog.n.01 with ' + s2 : wn.wup_similarity(dog, wn.synset(s2)) for s2 in list2}\n",
    "\n",
    "wu_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "582e6bf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 'cat.n.01')\n",
      "(1, 'crab.n.01')\n",
      "(2, 'spoon.n.01')\n",
      "(3, 'wolf.n.01')\n",
      "(4, 'puppy.n.01')\n"
     ]
    }
   ],
   "source": [
    "for s2 in enumerate(list2):\n",
    "    print(s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f59808bb",
   "metadata": {},
   "source": [
    "## 4 Sentiment analysis\n",
    "Son las técnicas que se usan para saber la connotación de una palabra, si es negativa, positiva o neutra. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "238b6000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\Magnus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "nltk.download('sentiwordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4bc8e880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emotions experienced when not in a state of well-being'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn.synsets('sadness')[0].definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b88f5a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sadness = swn.senti_synset('sadness.n.01')\n",
    "sadness.pos_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "20fc9fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "print(sadness.neg_score())\n",
    "print(sadness.obj_score())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "871e1d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "clock = swn.senti_synset('clock.n.01')\n",
    "clock.pos_score()\n",
    "print(clock.neg_score())\n",
    "print(clock.neg_score())\n",
    "print(clock.obj_score())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0a2a15",
   "metadata": {},
   "source": [
    "## 5. Word Embedding\n",
    "Cuando queremos entrenar un modelo de machine learning, necesitaremos pasar las palabras a números. Se pueden usar las relaciones citadas arriba, pero a veces es necesario incluir el texto en sí, como en un chatbot. Para trabajar con texto hay distintas técnicas pero la más utilizada ahora mismo es *word embedding*. Usaremos aquí el modelo de `Word2Vec`. Este modelo pasa una palabra a un vector, pero ha sido entrenado para que aprenda de palabras parecidas.  De tal manera que palabras parecidas estén cerca en el espacio vectorial. \n",
    "\n",
    "podéis ver algo más [aquí](https://medium.com/@zafaralibagh6/a-simple-word2vec-tutorial-61e64e38a6a1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f900ee4a",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3577734032.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[82], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gensim\n",
    "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06de820",
   "metadata": {},
   "source": [
    "Usamos un modelo preentrenado con datos de Google news:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a351678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "wv = api.load('word2vec-google-news-300')\n",
    "for index, word in enumerate(wv.index_to_key):\n",
    "    if index == 10:\n",
    "        break\n",
    "    print(f\"word #{index}/{len(wv.index_to_key)} is {word}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231419ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_king = wv['king']\n",
    "vec_king"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8452838f",
   "metadata": {},
   "source": [
    "Puede dar errores en palabras alejadas de las vistas en el training set.\n",
    "\n",
    "Al haber definido un espacio vectorial podemos calcular distancias entre palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4601d4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    ('dog', 'puppy'),   \n",
    "    ('dog', 'wolf'),   \n",
    "    ('dog', 'cat'),     \n",
    "    ('dog', 'crab'),    \n",
    "    ('dog', 'spoon'),\n",
    "]\n",
    "for w1, w2 in pairs:\n",
    "    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab4af1",
   "metadata": {},
   "source": [
    "## Ejemplo: NLP techinques + Redes neuronales (u otro clasificador de ML)\n",
    "Tenemos un data set con las entradas que ponemos en un buscador, las respuestas que nos da (título y descripción), y  como de bueno es el resultado de la búsqueda del 1,2, 3 y 4. Donde 4 es que ha acertado y 1 que no se parece en nada. ¿Cómo lo haríais?  \n",
    "\n",
    "<img src = img/crowdflower.png />\n",
    "\n",
    "https://www.kaggle.com/c/crowdflower-search-relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f889a7d",
   "metadata": {},
   "source": [
    "## Ejercicios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f245a88",
   "metadata": {},
   "source": [
    "1. (ejercicio 3.1 notebook anterior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1391cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codigo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e130a31",
   "metadata": {},
   "source": [
    "2. Lee el siguiente archivo data/text_ner.txt y busca la entidad de los nombre que salen en él."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25da678d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aebee2fe",
   "metadata": {},
   "source": [
    "3. Busca el primer nombre (NN o NNS) y el primer verbo (VB) del siguiente mismo texto y para cada uno encuentra la acepción que corresponde a ese contexto y escribe la definición y un sinónimo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26af31f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = \" Richard and Sonia Muller make documentaries about wildlife, particularly dangerous animals, like the big cats found in Africa. Film-making for them is a way to bring the message of the importance of understanding wildlife to international audiences, with their last film, Staying Alive, exploring relationships between lions and other wildlife in one African region. When Richard and Sonia were invited to help with a special project run by a wildlife organisation that was providing information about the falling numbers of big cats, especially lions, they immediately agreed to take part.\"\n",
    "texto += \"Richard grew up near a wildlife park and as a child was keen on filming what he saw. The couple were introduced at university in Cape Town, and quickly realised how much they had in common. They were both curious about the natural world and Sonia soon discovered a similar talent for filmmaking. As a child in South Africa Sonia often ran off alone to explore the wild areas surrounding her home, despite her parents’ fears.\"\n",
    "texto += \"When asked what they found hardest about their work, Sonia and Richard have the same answer - leaving an area and finishing a project. Sonia adds that the hours required can be hard, and things like the heat, dust, and bugs make it very tiring. The excitement of her work comes from not knowing what will happen, perhaps even discovering something new for science, while Richard takes most interest in spending time with individual animals, getting to know their character.\"\n",
    "texto += \"The pair visit schools around the world, and notice that students with access to lots of information don’t always have as much understanding about geography as students in countries where access is limited. “Students without the internet constantly available actually look at maps, they want to find out where they are and often end up with a better idea of place,” Richard says. A major part of their work is explaining to students the importance of a fuller understanding of various environments by studying the climate, animals and culture of a specific location.\"\n",
    "texto += \"If you’d like a similar career, Richard suggests studying various different areas of biology, rather than learning about the latest film- making technology, as an understanding of the natural world will last forever. The couple also give general advice for those wanting to help protect the environment. Sonia explains that it’s important to allow yourself to concentrate. “Turning off personal electronic items gets you closer to the natural world,” she says. “You can watch nature, instead of listening for your mobile phone.” Most importantly they agree that if urgent action isn’t taken, more animals might be lost. However, the fact that more teenagers are getting involved offers some hope for the future.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#código"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb078b3d",
   "metadata": {},
   "source": [
    "4. Definir una función que de una idea de si un texto es en general positivo o negativo. Usa distintas frases para comprobarlo.  (Es dificil hacer algo que funcione bien con poco rato, pero como ejercicio nos sirve) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ee7584",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
